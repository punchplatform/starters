{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3549b56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 17:59:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:SparkMonitorKernel:Client Connected ('127.0.0.1', 54162)\n"
     ]
    }
   ],
   "source": [
    "%%punch_spark_session\n",
    "{\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": \"http://minio.object-store:9000\",\n",
    "    \"spark.hadoop.fs.s3a.access.key\": \"minioadmin\",\n",
    "    \"spark.hadoop.fs.s3a.secret.key\": \"minioadmin\",\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe957d60",
   "metadata": {},
   "source": [
    "### Generate data to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4771df25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is available in df variable.\n"
     ]
    }
   ],
   "source": [
    "%%punch_source --type generator --name df -o\n",
    "messages:\n",
    "        - data: My first message\n",
    "        - data: My second message\n",
    "        - data: My third one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1e711d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(data='My first message'), Row(data='My second message')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56701c",
   "metadata": {},
   "source": [
    "### ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b21cb",
   "metadata": {},
   "source": [
    "##### Save to elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a085cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%punch_sink --type elasticsearch -df df\n",
    "http_hosts:\n",
    "  - host: elasticsearch-master.doc-store\n",
    "    port: 9200\n",
    "    scheme: http\n",
    "index:\n",
    "    type: constant\n",
    "    value: my-index-pyspark\n",
    "security: # Default null \n",
    "  # Credentials, token or basic\n",
    "  credentials: # Default null\n",
    "    # Option 1 : Basic\n",
    "    username: \"elastic\"\n",
    "    password: \"elastic\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53a5ef",
   "metadata": {},
   "source": [
    "#####  Fetch data from elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e03603a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is available in out variable.\n"
     ]
    }
   ],
   "source": [
    "%%punch_source --type elasticsearch --name out -o\n",
    "http_hosts:\n",
    "  - host: elasticsearch-master.doc-store\n",
    "    port: 9200\n",
    "    scheme: http \n",
    "security: # Default null \n",
    "  # Credentials, token or basic\n",
    "  credentials: # Default null\n",
    "    # Option 1 : Basic\n",
    "    username: \"elastic\"\n",
    "    password: \"elastic\"\n",
    "\n",
    "# Source settings\n",
    "index: my-index-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42203383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data='My second message'), Row(data='My third one')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed8e084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb5bf4",
   "metadata": {},
   "source": [
    "### Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572d045",
   "metadata": {},
   "source": [
    "##### Save to kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b28c593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%punch_sink --type kafka -df df\n",
    "kafka.bootstrap.servers: kooker-kafka-kafka-bootstrap.processing:9092\n",
    "topic: my-topic-pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08833eff",
   "metadata": {},
   "source": [
    "### Fetch data from kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce857931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is available in out variable.\n"
     ]
    }
   ],
   "source": [
    "%%punch_source --type kafka --name out -o\n",
    "kafka.bootstrap.servers: kooker-kafka-kafka-bootstrap.processing:9092\n",
    "topic: my-topic-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abe3ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(key=None, value=bytearray(b'{\"data\":\"My second message\"}'), topic='my-topic-pyspark', partition=0, offset=63, timestamp=datetime.datetime(2022, 10, 18, 14, 12, 2, 559000), timestampType=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39e88b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "del out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9000a",
   "metadata": {},
   "source": [
    "### File (s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981ec17",
   "metadata": {},
   "source": [
    "##### Save to file in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08ebdca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/19 17:59:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "22/10/19 17:59:46 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved.\n"
     ]
    }
   ],
   "source": [
    "%%punch_sink --type file -df df\n",
    "options:\n",
    "    header: True\n",
    "format: csv\n",
    "path: s3a://default/subdirectory/test.csv\n",
    "save_mode: Overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df039930",
   "metadata": {},
   "source": [
    "##### Recover data from s3 (file node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ad31e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is available in s3 variable.\n"
     ]
    }
   ],
   "source": [
    "%%punch_source --type file --name s3 -o \n",
    "options:\n",
    "    header: True\n",
    "path: s3a://default/subdirectory/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25aa4721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='0', data='My first message'), Row(_c0='1', data='My second message')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Punch Pyspark",
   "language": "python",
   "name": "python3_punch_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "1bfdf5a20b6fad5a24b66fafe35178c338447a5fbc205b80b5a132f6d64d4f63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
